{
    "nodes": [
        {
            "id": "Batory1988_GENESIS",
            "title": "GENESIS",
            "longtitle": "The GENESIS tool",
            "description": "GENESIS: an extensible database management system.",
            "concepts": [
                "The GENESIS tool"
            ],
            "hierarchy": [
                "Fundamentals",
                "Feature model's origins"
            ],
            "parents": [],
            "citations": 248
        },
        {
            "id": "Biggerstaff1989_ROSE",
            "title": "ROSE",
            "longtitle": "The ROSE reuse system",
            "description": "Design recovery for maintenance and reuse. The ROSE tool supported 'feature-based selection' of components. Feature-orientation can be traced back to the ROSE tool and the KAPTUR system.",
            "concepts": [
                "Feature-based selection of components"
            ],
            "hierarchy": [
                "Fundamentals",
                "Feature model's origins"
            ],
            "parents": [
                "Lubars1988_ROSE"
            ],
            "citations": 405
        },
        {
            "id": "Kang1990_FODA",
            "title": "FODA",
            "longtitle": "Feature-Oriented Domain Analysis (FODA)",
            "description": "Feature modeling was originally proposed as part of the Feature-Oriented Domain Analysis (FODA) method. Historically, FODA builds on, among others, Neighbors's work on Draco, and Batory's domain analysis of DBMS and the Genesis tool.",
            "concepts": [
                "Feature",
                "Feature model",
                "Feature diagram"
            ],
            "hierarchy": [
                "Fundamentals",
                "Feature model's origins"
            ],
            "parents": [
                "Neighbors1984_Draco",
                "Batory1988_GENESIS",
                "Moore1989_KAPTUR",
                "Biggerstaff1989_ROSE"
            ]
        },
        {
            "id": "Lubars1988_ROSE",
            "title": "ROSE",
            "longtitle": "The ROSE reuse system",
            "description": "Wide-spectrum support for software reusability.The ROSE tool supported 'feature-based selection' of components. Feature-orientation can be traced back to the ROSE tool and the KAPTUR system.",
            "concepts": [
                "Feature-based selection of components"
            ],
            "hierarchy": [
                "Fundamentals",
                "Feature model's origins"
            ],
            "parents": []
        },
        {
            "id": "Moore1989_KAPTUR",
            "title": "KAPTUR",
            "longtitle": "The KAPTUR system",
            "description": "The KAPTUR Environment: An Operations Concept. The KAPTUR system used 'distinctive features' to distinguish systems within a domain. Feature-orientation can be traced back to the ROSE tool and the KAPTUR system.",
            "concepts": [
                "Distinctive features"
            ],
            "hierarchy": [
                "Fundamentals",
                "Feature model's origins"
            ],
            "parents": []
        },
        {
            "id": "Neighbors1984_Draco",
            "title": "Draco",
            "longtitle": "The Draco approach",
            "description": "The Draco approach to constructing software from reusable components.",
            "concepts": [
                "The Draco approach"
            ],
            "hierarchy": [
                "Fundamentals",
                "Feature model's origins"
            ],
            "parents": [],
            "citations": 480
        },
        {
            "id": "Horcas2021_MCTS",
            "title": "MCTS for AAFM",
            "longtitle": "Monte Carlo Tree Search for Feature Model Analyses: a General Framework for Decision-Making",
            "description": "This work introduces the first general framework applying Monte Carlo Tree Search (MCTS) to the Automated Analysis of Feature Models (AAFM). It represents a significant shift from traditional deterministic approaches (like SAT or CSP solvers) toward simulation-based methods. By modeling feature model analysis as a sequential decision-making process, the framework allows for efficient exploration of colossal configuration spaces, providing a scalable alternative for problems such as finding defective configurations, reverse engineering, and performance optimization.",
            "concepts": [
                "Monte Carlo Tree Search (MCTS)"
            ],
            "hierarchy": [
                "Automated Analysis",
                "Search-based and Simulation Methods"
            ],
            "parents": [
                "Benavides2010_AAFM20yearsSLR"
            ],
            "citations": 10
        },
        {
            "id": "Horcas2021_Montecarlo",
            "title": "MC Simulations",
            "longtitle": "Monte Carlo Simulations for Variability Analyses in Highly Configurable Systems",
            "description": "This work introduces a simulation-based approach to variability analysis that applies Monte Carlo methods to estimate the influence of feature selections on configuration properties. To address the scalability limits of exhaustive exploration and traditional sampling, the approach treats configuration as a sequence of step-wise decisions centered on variation points. It leverages the law of large numbers to provide statistical estimations of how specific feature choices, such as selecting a particular database or framework, impact global properties like performance or defect probability, facilitating evidence-based recommendation systems.",
            "concepts": [
                "Monte Carlo simulations"
            ],
            "hierarchy": [
                "Automated Analysis",
                "Search-based and Simulation Methods"
            ],
            "parents": [
                "Horcas2021_MCTS",
                "Heradio2019_StatisticalAnalysisVM"
            ],
            "awards": [
                "üèÜBest Paper Award"
            ]
        },
        {
            "id": "Horcas2023_MCTSFramework",
            "title": "MCTS framework",
            "longtitle": "A Monte Carlo Tree Search conceptual framework for feature model analyses",
            "description": "This work presents the comprehensive and consolidated conceptual framework for applying Monte Carlo Tree Search (MCTS) to the Automated Analysis of Feature Models (AAFM). It formally defines how various variability analysis problems can be mapped onto Markov Decision Processes (MDPs). While previous milestones introduced the initial idea and specific simulation techniques, this publication serves as the definitive reference that unifies these methods into a single, scalable, and domain-independent framework capable of handling colossal configuration spaces where traditional exact solvers (SAT, BDD, CSP) become intractable.",
            "concepts": [
                "Monte Carlo Tree Search (MCTS)",
                "Markov Decision Process (MDP)",
                "Exploitation vs. Exploration (UCT)"
            ],
            "hierarchy": [
                "Automated Analysis",
                "Search-based and Simulation Methods"
            ],
            "parents": [
                "Horcas2021_MCTS",
                "Horcas2021_Montecarlo"
            ],
            "citations": 10
        },
        {
            "id": "Benavides2010_AAFM20yearsSLR",
            "title": "AAFM",
            "longtitle": "Automated analysis of feature models 20 years later",
            "description": "This is the first comprehensive literature review on the automated analysis of Feature Models (AAFM), covering research from the 20 years following their invention in 1990. Its historical importance lies in its role as a key reference point and conceptual foundation for the AAFM field. The authors synthesized previously disparate streams of work, defined a conceptual framework for the analysis process, and systematically classified 30 operations of analysis (e.g., void models, dead features, optimization) and four groups of automated techniques (i.e., Propositional Logic, Constraint Programming, Description Logic, and Other Analysis Techniques) used in the SPL community. The work also established several open challenges for future research, guiding the community for the next decade.",
            "concepts": [
                "Automated Analysis of Feature Models (AAFM)"
            ],
            "hierarchy": [
                "Automated Analysis"
            ],
            "parents": [
                "Benavides2005_AutomatedReasoning",
                "Batory2005_FMsGrammarsAndPropositionalFormulas"
            ],
            "citations": 1279
        },
        {
            "id": "Heradio2019_StatisticalAnalysisVM",
            "title": "Statistical Analysis",
            "longtitle": "Supporting the Statistical Analysis of Variability Models",
            "description": "This landmark paper establishes a formal probabilistic foundation for the automated analysis of variability models, moving the field beyond traditional binary (yes/no) logic. It introduces two exact and scalable algorithms based on Binary Decision Diagrams (BDDs): Feature Inclusion Probability (FIP), which calculates the likelihood of a feature being included in any valid product, and Product Distribution (PD), which computes the frequency of products based on their size. This work enables the characterization of software product lines through descriptive statistics (mean, variance, homogeneity) and provides a quantitative way to detect nearly-dead features and assess the impact of configuration choices in models with tens of thousands of features.",
            "concepts": [
                "Feature Inclusion Probability (FIP)",
                "Product Distribution (PD)"
            ],
            "hierarchy": [
                "Automated Analysis",
                "Statistical and Probabilistic Methods"
            ],
            "parents": [
                "Bryant1986_GraphBasedBDD"
            ],
            "citations": 23
        },
        {
            "id": "Benavides2005_AutomatedReasoning",
            "title": "Automated Reasoning on FMs",
            "longtitle": "Automated Reasoning on Feature Models",
            "description": "A major historical milestone as the first proposal to successfully model and reason on both functional and extra-functional features (i.e., quality or non-functional features) within a single feature model. Its key importance lies in introducing Constraint Programming (CP) and Constraint Satisfaction Optimization Problems (CSOP) as the formal foundation for automated analysis, alongside Propositional Logic. This work extended traditional FMs to include measurable attributes and attribute-based constraints. Furthermore, it generalized the capabilities of automated reasoning by defining a suite of five core operations (counting, filtering, listing products, validation, and optimization) that were more advanced than previous limited attempts. This established the paradigm for product-line optimization based on quality attributes like cost or development time.",
            "concepts": [
                "Attributed Feature Model",
                "Extended Feature Model"
            ],
            "hierarchy": [
                "Automated Analysis",
                "Constraint Programming based Analyses"
            ],
            "parents": [
                "Mannion2002_FirstOrderLogic",
                "Deursen2002_FDL"
            ],
            "awards": [
                "üèÜMost Influential Paper Award at SPLC 2017"
            ]
        },
        {
            "id": "Czarnecki2004_StagedConfiguration",
            "title": "Staged Configuration",
            "longtitle": "Staged Configuration of Feature Models",
            "description": "Historically, this work and its journal extension unified and formalized many disparate extensions into the Cardinality-Based Feature Model (CBFM) notation, which integrated feature cardinalities, group cardinalities, attributes, and diagram references. More critically, the work introduced the novel concept of Staged Configuration, acknowledging that in realistic development processes, different groups and roles (e.g., system administrator vs. end-user) eliminate product variability at different times or stages of the product lifecycle. This particular contribution at SPLC 2004 proposed achieving staged configuration through stepwise specialization (transforming a feature model into a less variable feature model).",
            "concepts": [
                "Staged configuration",
                "Stepwise specialization",
                "Cardinality-based feature model"
            ],
            "hierarchy": [
                "Configuration"
            ],
            "parents": [
                "Czarnecki2000_GenerativeProgramming"
            ],
            "awards": [
                "üèÜMost Influential Paper Award at SPLC 2017"
            ],
            "citations": 545
        },
        {
            "id": "Czarnecki2000_GenerativeProgramming",
            "title": "Generative Programming",
            "longtitle": "Generative Programming: Methods, Tools, and Applications",
            "description": "Generative programming is a software engineering paradigm aimed at improving productivity and quality by using automated techniques to generate software artifacts.",
            "concepts": [
                "Staged configuration",
                "Stepwise specialization",
                "Cardinality-based feature model"
            ],
            "hierarchy": [
                "Fundamentals",
                "FODA extensions"
            ],
            "parents": [
                "Kang1990_FODA"
            ],
            "awards": []
        },
        {
            "id": "Czarnecki2005_CardinalityFMs",
            "title": "Cardinality-based FMs",
            "longtitle": "Cardinality-based Feature Models",
            "description": "This work focuses on formalizing cardinality-based feature models and their specialization.",
            "concepts": [
                "Cardinality-based feature model"
            ],
            "hierarchy": [
                "Fundamentals",
                "FODA extensions"
            ],
            "parents": [
                "Czarnecki2000_GenerativeProgramming"
            ],
            "awards": []
        },
        {
            "id": "Czarnecki2005_StagedConfiguration",
            "title": "Multi-level configuration",
            "longtitle": "Multi-level Configuration of Feature Models",
            "description": "This work extended the Staged Configuration concept with Multilevel Configuration, where separate feature models define the choices for each stage. This multilevel approach was specifically proposed as a solution to avoid \"analysis paralysis\" by separating configuration choices based on abstraction levels.",
            "concepts": [
                "Staged configuration",
                "Multi-level configuration",
                "Cardinality-based feature model"
            ],
            "hierarchy": [
                "Configuration"
            ],
            "parents": [
                "Czarnecki2004_StagedConfiguration"
            ],
            "awards": []
        },
        {
            "id": "Batory2005_FMsGrammarsAndPropositionalFormulas",
            "title": "FMs, Grammars, and Propositional Formulas",
            "longtitle": "FMs, Grammars, and Propositional Formulas",
            "description": "Don Batory established a fundamental technical foundation for FMs by formally proving the connection between FMs, grammars, and propositional formulas. This formal equivalence was historically important because it provided a way to leverage mature, efficient tools from the logic and Artificial Intelligence communities to solve FM problems. Specifically, Batory proposed and demonstrated the use of Logic Truth Maintenance Systems (LTMSs) for real-time constraint propagation during product configuration and employed Satisfiability (SAT) solvers for debugging and verifying feature models. This approach established the boolean constraint satisfaction problem as the dominant technical paradigm for automated FM analysis.",
            "concepts": [
                "Propositional logic mapping",
                "SAT solvers for FM",
                "Feature Model Debugging",
                "Staged configuration",
                "Grammar-logic equivalence"
            ],
            "hierarchy": [
                "Automated Analysis",
                "Propositional logic based analyses"
            ],
            "parents": [
                "Czarnecki2005_StagedConfiguration",
                "Benavides2005_AutomatedReasoning",
                "Mannion2002_FirstOrderLogic"
            ],
            "awards": [
                "üèÜSPLC Test-of-Time Award 2017"
            ],
            "citations": 1366
        },
        {
            "id": "Mannion2002_FirstOrderLogic",
            "title": "First-Order Logic",
            "longtitle": "First-Order Logic for FM validation",
            "description": "This is a foundational work that introduced one of the earliest formal, automated methods for validating the consistency of SPL models. Its historical significance lies in establishing the technical paradigm of converting the complex dependency and variability relationships (like mutual exclusion or alternatives) into a single Propositional Logic expression. This technique allowed the automated verification of model properties, such as checking for void models (i.e., whether at least one valid system can be selected), and the validation of specific product configurations by using propositional calculus. Furthermore, the work formally posed key questions that later became the standard automated analysis operations in the FM community (e.g., product counting, product listing).",
            "concepts": [
                "Propositional connectives",
                "Logical expression",
                "Product line validation",
                "Configuration satisfiability",
                "Formal methods in SPL"
            ],
            "hierarchy": [
                "Automated Analysis",
                "Propositional logic based analyses"
            ],
            "parents": [],
            "awards": [],
            "citations": 264
        },
        {
            "id": "Deursen2002_FDL",
            "title": "FDL",
            "longtitle": "Feature Description Language (FDL)",
            "description": "This is a foundational work that transitioned FMs from a largely informal graphical tool to a formally verifiable and manipulable artifact. Its primary contribution was the proposal of the Feature Description Language (FDL), a textual Domain-Specific Language (DSL) designed to formally capture the variability expressed in Feature Diagrams. The historical significance lies in the introduction of the Feature Diagram Algebra, a suite of formal algebraic rules that enabled automated manipulation and analysis of FMs for the first time. This work established a concrete, tool-supportable approach to FM analysis by defining rules for normalization, variability calculation, and constraint checking.",
            "concepts": [
                "Feature Description Language (FDL)",
                "Feature Diagram Algebra"
            ],
            "hierarchy": [
                "Automated Analysis"
            ],
            "parents": [],
            "awards": [],
            "citations": 360
        },
        {
            "id": "Prehofer1997_FOP",
            "title": "FOP",
            "longtitle": "Feature-Oriented Programming (FOP)",
            "description": "This milestone marks the introduction and formalization of Feature-Oriented Programming (FOP). This work first proposed the concept, defining features as fine-grained increments of functionality similar to (abstract) subclasses but focused on the core functionality, with feature interactions (method overwrites) being specified separately. The core idea was to enable an exponential number of product combinations from a base set of features and quadratic interaction resolutions, going beyond standard object-oriented composition mechanisms like inheritance.",
            "concepts": [
                "Feature-oriented programming (FOP)"
            ],
            "hierarchy": [
                "Variability Implementation",
                "Composition-based approaches",
                "Feature-oriented programming (FOP)"
            ],
            "parents": [],
            "awards": [
                "üèÜMost Influential Paper Award at SPLC 2018"
            ],
            "citations": 527
        },
        {
            "id": "Prehofer2001_FOP",
            "title": "FOP",
            "longtitle": "Feature-Oriented Programming (FOP)",
            "description": "This work extended and formalized the Feature-Oriented Programming (FOP) model, specifically introducing explicit interaction handlers to adapt features and address the feature interaction problem systematically. Historically, this work is crucial as it connected feature models from the domain analysis phase (variability modeling) directly to the implementation phase (code composition), establishing one of the major paradigms, alongside Aspect-Oriented Programming (AOP), for realizing software product lines at the source code level.",
            "concepts": [
                "Feature-oriented programming (FOP)"
            ],
            "hierarchy": [
                "Variability Implementation",
                "Composition-based approaches",
                "Feature-oriented programming (FOP)"
            ],
            "parents": [
                "Prehofer1997_FOP"
            ],
            "awards": [
                "üèÜMost Influential Paper Award at SPLC 2018"
            ],
            "citations": 45
        },
        {
            "id": "Batory2004_AHEAD",
            "title": "AHEAD",
            "longtitle": "Feature-oriented programming (FOP) and the AHEAD tool suite",
            "description": "This work focuses on feature-oriented programming and the AHEAD tool suite.",
            "concepts": [
                "Feature-oriented programming (FOP)",
                "AHEAD"
            ],
            "hierarchy": [
                "Variability Implementation",
                "Feature-oriented programming (FOP)"
            ],
            "parents": [
                "Prehofer2001_FOP"
            ],
            "citations": 219
        },
        {
            "id": "K√§stner2008_CIDE",
            "title": "CIDE",
            "longtitle": "Granularity in Software Product Lines with Colored IDE (CIDE)",
            "description": "This work addressed the critical gap between Feature Models (specifying variability at an abstract level) and Feature Implementation (realizing variability in code). Historically, existing implementation approaches (like FOP, AOP, or C-preprocessor) primarily supported coarse-grained features (e.g., adding/wrapping entire classes or methods). This work showed that fine-grained extensions (e.g., adding a single statement or expression inside a method body) are essential, particularly when extracting features from legacy systems or for minimizing code replication. The key contribution was the introduction of Colored IDE (CIDE), a tool that allowed developers to implement both coarse and fine-grained extensions in a clean, concise, and separated manner by coloring code elements according to the feature they belong to. CIDE thus offered a fundamental implementation mechanism that directly supported the full variability complexity captured in Feature Models.",
            "concepts": [
                "Feature Granularity, Colored IDE (CIDE)"
            ],
            "hierarchy": [
                "Variability Implementation"
            ],
            "parents": [
                "Batory2004_AHEAD",
                "Smaragdakis2002_MixinLayers",
                "Apel2008_AspectualFeatureModules",
                "Kiczales1997_AOP"
            ],
            "awards": [
                "üèÜMost Influential Paper Award at SPLC 2019"
            ],
            "citations": 510
        },
        {
            "id": "Smaragdakis2002_MixinLayers",
            "title": "Mixin Layers",
            "longtitle": "Mixin Layers",
            "description": "This work introduced Mixin Layers as a robust, object-oriented technique for implementing large-scale refinements and collaboration-based designs. Its historical importance is that it provided one of the most powerful and flexible mechanisms for realizing the variability captured in Feature Models at the source code level, offering a practical alternative to Aspect-Oriented Programming (AOP) and earlier feature-oriented approaches. A Mixin Layer is a collection of classes that together implement a feature or refinement, and they are composable in a sequence (a stack) to build the final product. The composition mechanism, which avoids class-naming conflicts, allows for non-invasive feature addition and provides a clean separation of concerns, thereby directly supporting the principles of Software Product Line engineering.",
            "concepts": [
                "Mixin Layers",
                "Refinements as Layers",
                "Collaboration-Based Design",
                "Non-Invasive Composition"
            ],
            "hierarchy": [
                "Variability Implementation",
                "Composition-based approaches"
            ],
            "parents": [],
            "citations": 363
        },
        {
            "id": "Apel2008_AspectualFeatureModules",
            "title": "Aspectual Feature Modules",
            "longtitle": "Aspectual Feature Modules",
            "description": "This work is a significant milestone that unified the two dominant variability realization paradigms: Feature-Oriented Programming (FOP) and Aspect-Oriented Programming (AOP). Historically, FOP (using Feature Modules) was excellent for implementing coarse-grained, compositional software blocks, while AOP (using Aspects) was essential for modularizing fine-grained, crosscutting concerns (e.g., logging, security) that scatter across multiple modules. This work systematically evaluated the strengths and weaknesses of both, concluding that they are complementary. The key contribution was the introduction of Aspectual Feature Modules (AFMs), a novel programming technique that integrates Feature Modules and Aspects into a unified construct. This provided software product line (SPL) developers with a single, comprehensive mechanism to implement both the large-scale feature logic (the FOP part) and the pervasive, non-functional requirements (the AOP part), thereby improving modularity and maintainability in SPLs.",
            "concepts": [
                "Aspectual Feature Modules",
                "Complementarity of FOP and AOP",
                "Holistic Variability Realization"
            ],
            "hierarchy": [
                "Variability Implementation",
                "Composition-based approaches"
            ],
            "parents": [],
            "citations": 192
        },
        {
            "id": "Classen2010_TemporalPropertiesSPLs",
            "title": "Model Checking SPLs",
            "longtitle": "Efficient Verification of Temporal Properties in SPLs",
            "description": "A major historical advance in the formal verification of Software Product Lines (SPLs). Given that the number of products in an SPL can be exponential in the number of features (the \"combinatorial explosion\" problem), previous approaches to verify system behavior (e.g., model checking each product individually) were not scalable. This work's historical importance is that it introduced a scalable technique for verifying temporal properties (e.g., \"The system will eventually reach a safe state\") across the entire family of systems simultaneously. The key was to extend transition systems with features (Feature-Oriented Transition Systems) and apply a specialized lifted model checking algorithm. This allowed checking a single compact model, rather than 2N product-specific models, making the formal verification of system behavior in industrial-scale SPLs feasible for the first time.",
            "concepts": [
                "Feature-Oriented Transition Systems (FOTS)",
                "Lifted Model Checking",
                "Behavioral Verification"
            ],
            "hierarchy": [
                "Automated Analysis",
                "Formal Verification"
            ],
            "parents": [],
            "awards": [
                "üèÜMost Influential Paper Award at SPLC 2020"
            ],
            "citations": 375
        },
        {
            "id": "Schaefer2010_DOP",
            "title": "DOP",
            "longtitle": "Delta-Oriented Programming (DOP)",
            "description": "This work introduced Delta-Oriented Programming (DOP) as a novel, fundamental paradigm for implementing Software Product Lines (SPLs), offering an alternative to the compositional approach of Feature-Oriented Programming (FOP). Historically, FOP relied on extending existing code by stacking feature modules. DOP, by contrast, relies on a core module (a valid product) and a set of delta modules that specify additive, modificatory, and removal changes to the core code. This historical shift provides greater flexibility, as it supports modifications and removals that are cumbersome in purely compositional FOP approaches. Products are derived by applying a sequence of delta modules based on the feature configuration. DOP is particularly valuable for SPLs derived from legacy or existing single-application systems because it aligns closely with the patching/refactoring nature of such evolution.",
            "concepts": [
                "Delta-Oriented Programming (DOP)",
                "Core Module",
                "Delta Modules",
                "Application Conditions"
            ],
            "hierarchy": [
                "Variability Implementation",
                "Composition-based approaches",
                "Delta-Oriented Programming (DOP)"
            ],
            "parents": [],
            "awards": [
                "üèÜMost Influential Paper Award at SPLC 2022"
            ],
            "citations": 413
        },
        {
            "id": "Perrouin2010_TWiseTestCaseGeneration",
            "title": "T-wise Testing",
            "longtitle": "T-wise Test Case Generation",
            "description": "A foundational work in the testing and quality assurance of Software Product Lines (SPLs). Given the combinatorial explosion problem‚Äîwhere an SPL can have thousands or millions of valid products‚Äîexhaustive testing is infeasible. This work popularized the use of T-wise combinatorial testing as a practical and effective strategy for SPL validation. T-wise testing generates a small, representative subset of products that guarantees coverage of all possible interactions between T features. The historical importance of the paper lies in addressing the scalability and automation challenges of this method. It proposes and implements a toolset that uses the Alloy constraint solver to automatically generate T-wise test cases from Feature Models, and, critically, introduces strategies to split large T-wise combinations into solvable subsets to overcome the memory and time limitations of SAT solvers (\"all-at-once\" failure).",
            "concepts": [
                "T-wise Combinatorial Testing for SPLs",
                "Alloy-Based Test Generation",
                "Scalability Strategies",
                "Metrics for Evaluation"
            ],
            "hierarchy": [
                "Software Product Lines",
                "Testing"
            ],
            "parents": [],
            "awards": [
                "üèÜMost Influential Paper Award at SPLC 2024"
            ]
        },
        {
            "id": "Thum2009_ReasoningAboutEditsToFMs",
            "title": "Refactoring",
            "longtitle": "Reasoning about Edits to Feature Models",
            "description": "This work addresses the critical and practical problem of Feature Model (FM) evolution. Since Software Product Lines (SPLs) and their models evolve continuously, designers need to understand and control how modifications to an FM affect the set of valid products (the product space). The historical importance of this work lies in providing the first formal and efficient framework for reasoning about and classifying FM edits. The authors defined a taxonomy of edits (refactorings, specializations, generalizations, and arbitrary edits) based on whether the edit preserves, reduces, or expands the product space. Most crucially, they presented an algorithm that automatically takes two FMs (before and after edit) as input and efficiently computes the classification and determines the set of added or deleted products, even for large models with thousands of features. This capability provided essential tool support for maintaining consistency and coherence during SPL evolution.",
            "concepts": [
                "FM Edit Taxonomy",
                "Refactoring",
                "Specialization",
                "Generalization",
                "Change Reasoning Algorithm",
                "Automated Coherence Management"
            ],
            "hierarchy": [
                "Evolution"
            ],
            "parents": [],
            "awards": [
                "üèÜMost Influential Paper Award at SPLC 2023"
            ],
            "citations": 386
        },
        {
            "id": "Zhang2004_EquivalentFeatures",
            "title": "Equivalent Features",
            "longtitle": "Equivalent Features (Precursor to Atomic Sets)",
            "description": "This milestone focuses on the Atomic Set (AS) concept, a fundamental technique for simplifying Feature Models and improving the scalability of automated analysis. Zhang et al. first introduced the concept, implicitly recognizing that in any valid product configuration, certain features are always selected together or never selected at all. These sets of features that behave identically across all valid products were termed Equivalent Features and served as the foundational idea for Atomic Sets.",
            "concepts": [
                "Equivalent Features"
            ],
            "hierarchy": [
                "Automated Analysis",
                "Analysis Operations"
            ],
            "parents": [],
            "citations": 147
        },
        {
            "id": "Segura2008_AtomicSets",
            "title": "Atomic Sets",
            "longtitle": "Atomic Sets",
            "description": "S. Segura formalized and popularized the term Atomic Sets, proposing them as a generic technique for FM simplification. Its key contribution was to explicitly set the basis for the usage of Atomic Sets by defining specific algorithms and providing the first empirical evidence of their effectiveness in reducing the size of the propositional logic formula, thereby dramatically improving the performance of SAT-based analysis tools. The relationship between this contribution and its predecessor is that Zhang's work established the core idea (Equivalent Features), which Segura later formalized, named, and validated as a practical, scalable analysis technique (Atomic Sets).",
            "concepts": [
                "Atomic Sets",
                "FM Simplification/Variable Reduction",
                "Scalability for Automated Analysis"
            ],
            "hierarchy": [
                "Automated Analysis",
                "Analysis Operations"
            ],
            "parents": [
                "Zhang2004_EquivalentFeatures"
            ]
        },
        {
            "id": "CarmoMachado2014_SPLTestingSLR",
            "title": "SPL Testing SLR",
            "longtitle": "Software Product Line Testing: A Systematic Literature Review",
            "description": "This Systematic Literature Review is a foundational milestone in the quality assurance of SPLs. Its historical importance lies in being one of the first major reviews to specifically focus on testing strategies aimed at achieving economies of scope and scale in SPL testing. The review systematically identified and classified available research, providing a crucial synthesis for the community. It explicitly defined and contrasted key strategies such as selection-based testing (testing a subset of products, e.g., T-wise testing) and family-based testing (testing the product line assets directly), and assessed their potential for efficiency gains. This work formalized the landscape of SPL testing, setting the context for subsequent research on identifying and addressing persistent gaps.",
            "concepts": [
                "Testing Strategies Taxonomy",
                "Selection-Based Testing",
                "Family-Based Testing",
                "Economies of Scale and Scope"
            ],
            "hierarchy": [
                "Software Product Lines",
                "Testing"
            ],
            "parents": [],
            "citations": 155
        },
        {
            "id": "Agh2024_SPLTestingSLR",
            "title": "SPL Testing SLR",
            "longtitle": "Software Product Line Testing: A Systematic Literature Review",
            "description": "This Systematic Literature Review provides a crucial historical snapshot and comprehensive analysis of the entire field of SPL Testing. Its historical importance is that it synthesizes two decades of work (from 2003 to 2022), offering an up-to-date classification, assessment, and gap analysis for researchers and practitioners. The authors analyzed 118 studies to answer key questions, confirming the maturity of certain areas (e.g., T-wise testing and automated configuration selection) while highlighting significant deficiencies. Most notably, the review identifies a major gap in techniques for non-functional testing (e.g., performance, security) and regression testing in SPLs, guiding the next decade of research focus in quality assurance for product lines.",
            "concepts": [
                "Comprehensive Taxonomy of SPL Testing",
                "Maturity Assessment",
                "Research Gaps",
                "Empirical Evidence Analysis"
            ],
            "hierarchy": [
                "Software Product Lines",
                "Testing"
            ],
            "parents": [
                "CarmoMachado2014_SPLTestingSLR"
            ],
            "citations": 4
        },
        {
            "id": "Raatikainen2019_SPLVMTertiaryStudy",
            "title": "Tertiary Study",
            "longtitle": "Software product lines and variability modeling: A tertiary study",
            "description": "This Tertiary Study is a crucial historical reference for the SPL and FM community. A tertiary study analyzes and synthesizes the results of multiple systematic reviews and mapping studies. Its historical importance is that it summarized the state of research across the entire variability domain by synthesizing 86 existing reviews. The study characterized the landscape of existing systematic reviews, assessed their quality, and provided a multi-dimensional classification of variability models based on the development artifacts they cover (e.g., requirements, architecture, code). Crucially, this work confirmed the dominance of Feature Models across the lifecycle while identifying that research efforts were unevenly distributed, with a significant gap remaining in variability realization/implementation (i.e., the techniques for turning the models into actual code).",
            "concepts": [
                "Tertiary Study",
                "Variability Model Classification"
            ],
            "hierarchy": [
                "Variability Modeling"
            ],
            "parents": [],
            "citations": 94
        },
        {
            "id": "Eriksson2005_PLUSS",
            "title": "PLUSS",
            "longtitle": "The PLUSS approach",
            "description": "The PLUSS approach (Product Line Use case modeling for Systems and Software engineering) was developed to handle the complexities of software-intensive systems with extremely long lifespans (e.g., 30+ years in the defense industry). It refined the integration of feature models with use case-driven development. Historically, it builds on FeatuRSEB and the Rational Unified Process (RUP), but introduces a more rigorous method for tracing variability from high-level features down to UML use case realizations. It treats the feature model as the 'choice-master' that coordinates variation points across all other technical models.",
            "concepts": [
                "Product Line Use Cases (PLUC)",
                "Use Case Realization variability",
                "Feature-to-Model Traceability",
                "Long-lived systems engineering",
                "Variation Point modeling in UML"
            ],
            "hierarchy": [
                "Software Product Lines"
            ],
            "parents": [
                "Griss1998_FeatuRSEB"
            ],
            "citations": 175
        },
        {
            "id": "Griss1998_FeatuRSEB",
            "title": "FeatuRSEB",
            "longtitle": "Reusable Software Engineering Base (FeatuRSEB)",
            "description": "This seminal work integrates the feature modeling method of Feature-Oriented Domain Analysis (FODA) into the processes and workproducts of the Reuse-Driven Software Engineering Business (RSEB). Historically, it is significant for bridging the gap between requirements-oriented use cases and configuration-oriented feature models. It established the feature model as the central '+1' model that unifies different architectural views in domain engineering, serving as a 'catalog' and 'roadmap' for reusers to navigate and customize complex software product lines.",
            "concepts": [
                "Feature model as a catalog",
                "Use case vs. Feature relationship",
                "Variation points and variants",
                "Mandatory, Optional, and Variant features",
                "Reuser-oriented perspective",
                "'+1' model of domain engineering"
            ],
            "hierarchy": [
                "Fundamentals",
                "FODA extensions"
            ],
            "parents": [
                "Kang1990_FODA"
            ],
            "citations": 636
        },
        {
            "id": "Kang1998_FORM",
            "title": "FORM",
            "longtitle": "Feature-Oriented Reuse Method (FORM)",
            "description": "FORM (Feature-Oriented Reuse Method) is the primary evolution and extension of the original FODA (1990) method. Its historical significance lies in bridging the gap between domain analysis (identifying features) and software architecture (implementing features). It introduced a systematic process for both 'Domain Engineering' (creating reusable assets) and 'Application Engineering' (building products from assets). Most notably, it introduced the four-layer feature modeling hierarchy‚Äîcategorizing features into Capability, Operating Environment, Domain Technology, and Implementation Technique‚Äîproviding a structured way to map high-level user requirements to low-level technical decisions.",
            "concepts": [
                "Four-layer feature hierarchy",
                "Domain Engineering",
                "Application Engineering",
                "Reference Architecture",
                "Feature-architecture mapping"
            ],
            "hierarchy": [
                "Fundamentals",
                "FODA extensions"
            ],
            "parents": [
                "Kang1990_FODA"
            ],
            "citations": 1127
        },
        {
            "id": "Greenfield2004_SoftwareFactories",
            "title": "Software Factories",
            "longtitle": "Software Factories: Assembling Applications with Patterns, Models, Frameworks, and Tools",
            "description": "This work focuses on the assembly of applications using patterns, models, frameworks, and tools.",
            "concepts": [
                "Software factories"
            ],
            "hierarchy": [
                "Software Product Lines"
            ],
            "parents": [
                "Clements2001_SPLPracticesAndPatterns",
                "Tseng2001_MassCustomization"
            ]
        },
        {
            "id": "Clements2001_SPLPracticesAndPatterns",
            "title": "SPLs",
            "longtitle": "Software Product Lines: Practices and Patterns",
            "description": "This work focuses on the practices and patterns in software product lines.",
            "concepts": [
                "Software product line"
            ],
            "hierarchy": [
                "Software Product Lines"
            ],
            "parents": []
        },
        {
            "id": "Tseng2001_MassCustomization",
            "title": "Mass Customization",
            "longtitle": "Mass Customization",
            "description": "This work focuses on mass customization.",
            "concepts": [
                "Mass customization"
            ],
            "hierarchy": [
                "Software Product Lines"
            ],
            "parents": []
        },
        {
            "id": "Pohl2005_SPLEngineering",
            "title": "SPL Engineering",
            "longtitle": "Software Product Line Engineering - Foundations, Principles, and Techniques",
            "description": "This work focuses on the foundations, principles, and techniques of software product line engineering.",
            "concepts": [
                "Software product line engineering"
            ],
            "hierarchy": [
                "Software Product Lines"
            ],
            "parents": [
                "Greenfield2004_SoftwareFactories"
            ],
            "citations": 2891
        },
        {
            "id": "Sundermann2021_UVL",
            "title": "UVL Inception",
            "longtitle": "Yet Another Textual Variability Language? A Community Effort Towards a Unified Language",
            "description": "This paper introduces the Universal Variability Language (UVL), a community-driven initiative born from the MODEVAR workshop to address the fragmentation of textual feature modeling languages. It proposes UVL as a human-readable pivot format designed to facilitate model exchange between diverse researchers and practitioners. The work evaluates community preferences to ensure the language serves as a unifying standard rather than just another competing notation.",
            "concepts": [
                "Universal Variability Language (UVL)",
                "Pivot language",
                "Textual variability modeling",
                "MODEVAR initiative",
                "Human-readable exchange format"
            ],
            "hierarchy": [
                "Variability modeling",
                "Textual Variability Languages",
                "UVL"
            ],
            "parents": [
                "HterBeek2019_TextualLanguages",
                "Berger2019_UsageScenarios",
                "Thum2019_LanguageLevels"
            ],
            "citations": 62
        },
        {
            "id": "Sundermann2023_UVLParserExtensions",
            "title": "UVL Extensions",
            "longtitle": "UVLParser: Extending UVL with Language Levels and Conversion Strategies",
            "description": "This work introduces technical enhancements to the UVL ecosystem by extending the UVLParser with the concepts of language levels and conversion strategies. Language levels allow tools to explicitly define which UVL constructs they support (e.g., Boolean vs. Arithmetic), while conversion strategies provide the logic to automatically transform models between these levels. This enables interoperability between tools with varying degrees of analytical sophistication.",
            "concepts": [
                "Language Levels",
                "Conversion Strategies",
                "UVLParser",
                "Level-aware parsing",
                "Tool interoperability"
            ],
            "hierarchy": [
                "Variability modeling",
                "Textual Variability Languages",
                "UVL"
            ],
            "parents": [
                "Sundermann2021_UVL"
            ],
            "citations": 18
        },
        {
            "id": "Benavides2025_UVL",
            "title": "UVL Consolidation",
            "longtitle": "UVL: Feature modelling with the Universal Variability Language",
            "description": "As the definitive journal reference, this paper formalizes the syntax and semantics of the Universal Variability Language (UVL). It details the language's core structure across three primary levels‚ÄîBoolean, Arithmetic, and Type‚Äîwhile providing a robust mechanism for future extensions. The publication solidifies UVL's status as the community standard, documenting its broad adoption in both academic research and industrial applications.",
            "concepts": [
                "UVL Formalization",
                "Feature model meta-model",
                "Three-level structure (Boolean, Arithmetic, Type)",
                "UVL Extensibility",
                "Standardization"
            ],
            "hierarchy": [
                "Variability modeling",
                "Textual Variability Languages",
                "UVL"
            ],
            "parents": [
                "Sundermann2021_UVL",
                "Sundermann2023_UVLParserExtensions"
            ],
            "citations": 0
        },
        {
            "id": "HterBeek2019_TextualLanguages",
            "title": "TVL Survey",
            "longtitle": "Textual Variability Modeling Languages: An Overview and Considerations",
            "description": "This milestone provides a comprehensive survey of the fragmented landscape of textual variability languages (TVLs). By identifying and analyzing over 90 different languages used in research and industry, the authors highlighted a significant lack of standardization and interoperability. This work served as the empirical evidence needed to convince the community that 'yet another language' was not the solution, but rather a unified, common language was required to prevent further divergence in the field.",
            "concepts": [
                "Textual Variability Languages (TVL)",
                "Language fragmentation",
                "Interoperability gaps",
                "Language classification",
                "Survey of notations"
            ],
            "hierarchy": [
                "Variability modeling",
                "Textual Variability Languages"
            ],
            "parents": [],
            "citations": 43
        },
        {
            "id": "Berger2019_UsageScenarios",
            "title": "Common Language Scenarios",
            "longtitle": "Usage Scenarios for a Common Feature Modeling Language",
            "description": "This work defines the functional and non-functional requirements for a unified feature modeling language by identifying ten critical usage scenarios. These scenarios range from model exchange between different analysis tools to the use of feature models in teaching and industrial certification. Historically, this paper transitioned the discussion from 'what a language looks like' to 'what a language must do,' ensuring that the subsequent development of UVL was driven by practical, real-world needs rather than theoretical aesthetics.",
            "concepts": [
                "Usage scenarios",
                "Model exchange",
                "Tool interoperability",
                "Requirements engineering for DSLs",
                "Community standards"
            ],
            "hierarchy": [
                "Variability modeling",
                "Textual Variability Languages"
            ],
            "parents": [],
            "citations": 22
        },
        {
            "id": "Thum2019_LanguageLevels",
            "title": "Language Levels",
            "longtitle": "On Language Levels for Feature Modeling Notations",
            "description": "This milestone introduces the architectural concept of 'Language Levels' to manage the diversity of feature modeling notations. The authors proposed that a unified language should not be a monolithic entity but a tiered structure where different levels of expressiveness (e.g., basic Boolean constraints vs. complex attributes and types) can be supported by different tools. This conceptual breakthrough allowed for a modular language design where simple tools can remain simple while advanced tools can exchange highly complex models within the same syntax.",
            "concepts": [
                "Language levels",
                "Tiers of expressiveness",
                "Modular language design",
                "Feature model meta-modeling",
                "Extensibility"
            ],
            "hierarchy": [
                "Variability modeling",
                "Textual Variability Languages"
            ],
            "parents": [],
            "citations": 14
        },
        {
            "id": "Kiczales1997_AOP",
            "title": "AOP",
            "longtitle": "Aspect-Oriented Programming (AOP)",
            "description": "This work introduced the concept of Aspect-Oriented Programming (AOP) and the first implementation in the AspectJ language. Its historical significance for Feature Models (FMs) is profound: it provided the second major technical solution (alongside Feature-Oriented Programming) for the realization of variability in Software Product Lines (SPLs) at the code level. The core concept is the crosscutting concern (an aspect), which is a design decision (like logging, synchronization, or security) whose implementation logically belongs to a single module but is physically scattered across many parts of the system's core functionality (the base code). AOP allowed developers to modularize these crosscutting features, thus directly supporting the implementation of features that require modifications to many different parts of a software product line. This capability was later integrated with FOP to create unified realization techniques.",
            "concepts": [
                "Aspect-Oriented Programming (AOP)",
                "Aspects",
                "Crosscutting Concerns",
                "Tangled Code",
                "Join Point Model"
            ],
            "hierarchy": [
                "Variability Implementation",
                "Composition-based approaches",
                "Aspect-Oriented Programming (AOP)"
            ],
            "parents": []
        }
    ],
    "links": [
        {
            "source": "Lubars1988_ROSE",
            "target": "Biggerstaff1989_ROSE"
        },
        {
            "source": "Neighbors1984_Draco",
            "target": "Kang1990_FODA"
        },
        {
            "source": "Batory1988_GENESIS",
            "target": "Kang1990_FODA"
        },
        {
            "source": "Moore1989_KAPTUR",
            "target": "Kang1990_FODA"
        },
        {
            "source": "Biggerstaff1989_ROSE",
            "target": "Kang1990_FODA"
        },
        {
            "source": "Benavides2010_AAFM20yearsSLR",
            "target": "Horcas2021_MCTS"
        },
        {
            "source": "Horcas2021_MCTS",
            "target": "Horcas2021_Montecarlo"
        },
        {
            "source": "Heradio2019_StatisticalAnalysisVM",
            "target": "Horcas2021_Montecarlo"
        },
        {
            "source": "Horcas2021_MCTS",
            "target": "Horcas2023_MCTSFramework"
        },
        {
            "source": "Horcas2021_Montecarlo",
            "target": "Horcas2023_MCTSFramework"
        },
        {
            "source": "Benavides2005_AutomatedReasoning",
            "target": "Benavides2010_AAFM20yearsSLR"
        },
        {
            "source": "Batory2005_FMsGrammarsAndPropositionalFormulas",
            "target": "Benavides2010_AAFM20yearsSLR"
        },
        {
            "source": "Mannion2002_FirstOrderLogic",
            "target": "Benavides2005_AutomatedReasoning"
        },
        {
            "source": "Deursen2002_FDL",
            "target": "Benavides2005_AutomatedReasoning"
        },
        {
            "source": "Czarnecki2000_GenerativeProgramming",
            "target": "Czarnecki2004_StagedConfiguration"
        },
        {
            "source": "Kang1990_FODA",
            "target": "Czarnecki2000_GenerativeProgramming"
        },
        {
            "source": "Czarnecki2000_GenerativeProgramming",
            "target": "Czarnecki2005_CardinalityFMs"
        },
        {
            "source": "Czarnecki2004_StagedConfiguration",
            "target": "Czarnecki2005_StagedConfiguration"
        },
        {
            "source": "Czarnecki2005_StagedConfiguration",
            "target": "Batory2005_FMsGrammarsAndPropositionalFormulas"
        },
        {
            "source": "Benavides2005_AutomatedReasoning",
            "target": "Batory2005_FMsGrammarsAndPropositionalFormulas"
        },
        {
            "source": "Mannion2002_FirstOrderLogic",
            "target": "Batory2005_FMsGrammarsAndPropositionalFormulas"
        },
        {
            "source": "Prehofer1997_FOP",
            "target": "Prehofer2001_FOP"
        },
        {
            "source": "Prehofer2001_FOP",
            "target": "Batory2004_AHEAD"
        },
        {
            "source": "Batory2004_AHEAD",
            "target": "K√§stner2008_CIDE"
        },
        {
            "source": "Smaragdakis2002_MixinLayers",
            "target": "K√§stner2008_CIDE"
        },
        {
            "source": "Apel2008_AspectualFeatureModules",
            "target": "K√§stner2008_CIDE"
        },
        {
            "source": "Kiczales1997_AOP",
            "target": "K√§stner2008_CIDE"
        },
        {
            "source": "Zhang2004_EquivalentFeatures",
            "target": "Segura2008_AtomicSets"
        },
        {
            "source": "CarmoMachado2014_SPLTestingSLR",
            "target": "Agh2024_SPLTestingSLR"
        },
        {
            "source": "Griss1998_FeatuRSEB",
            "target": "Eriksson2005_PLUSS"
        },
        {
            "source": "Kang1990_FODA",
            "target": "Griss1998_FeatuRSEB"
        },
        {
            "source": "Kang1990_FODA",
            "target": "Kang1998_FORM"
        },
        {
            "source": "Clements2001_SPLPracticesAndPatterns",
            "target": "Greenfield2004_SoftwareFactories"
        },
        {
            "source": "Tseng2001_MassCustomization",
            "target": "Greenfield2004_SoftwareFactories"
        },
        {
            "source": "Greenfield2004_SoftwareFactories",
            "target": "Pohl2005_SPLEngineering"
        },
        {
            "source": "HterBeek2019_TextualLanguages",
            "target": "Sundermann2021_UVL"
        },
        {
            "source": "Berger2019_UsageScenarios",
            "target": "Sundermann2021_UVL"
        },
        {
            "source": "Thum2019_LanguageLevels",
            "target": "Sundermann2021_UVL"
        },
        {
            "source": "Sundermann2021_UVL",
            "target": "Sundermann2023_UVLParserExtensions"
        },
        {
            "source": "Sundermann2021_UVL",
            "target": "Benavides2025_UVL"
        },
        {
            "source": "Sundermann2023_UVLParserExtensions",
            "target": "Benavides2025_UVL"
        }
    ]
}